{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9532359,"sourceType":"datasetVersion","datasetId":5805408},{"sourceId":126194,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":106228,"modelId":130520}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-03T11:33:19.470442Z","iopub.execute_input":"2024-10-03T11:33:19.470906Z","iopub.status.idle":"2024-10-03T11:33:21.854056Z","shell.execute_reply.started":"2024-10-03T11:33:19.470865Z","shell.execute_reply":"2024-10-03T11:33:21.852815Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Добавим трекинг","metadata":{}},{"cell_type":"code","source":"!pip install clearml","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:31:45.612862Z","iopub.execute_input":"2024-10-03T11:31:45.613736Z","iopub.status.idle":"2024-10-03T11:31:59.222925Z","shell.execute_reply.started":"2024-10-03T11:31:45.613678Z","shell.execute_reply":"2024-10-03T11:31:59.221770Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: clearml in /opt/conda/lib/python3.10/site-packages (1.16.4)\nRequirement already satisfied: attrs>=18.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (23.2.0)\nRequirement already satisfied: furl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (2.1.3)\nRequirement already satisfied: jsonschema>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (4.22.0)\nRequirement already satisfied: numpy>=1.10 in /opt/conda/lib/python3.10/site-packages (from clearml) (1.26.4)\nRequirement already satisfied: pathlib2>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (2.3.7.post1)\nRequirement already satisfied: Pillow>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from clearml) (10.3.0)\nRequirement already satisfied: psutil>=3.4.2 in /opt/conda/lib/python3.10/site-packages (from clearml) (5.9.3)\nRequirement already satisfied: pyparsing>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from clearml) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.10/site-packages (from clearml) (2.9.0.post0)\nRequirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.10/site-packages (from clearml) (6.0.2)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (2.32.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (1.16.0)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from clearml) (1.26.18)\nRequirement already satisfied: pyjwt<2.9.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from clearml) (2.8.0)\nRequirement already satisfied: referencing<0.40 in /opt/conda/lib/python3.10/site-packages (from clearml) (0.35.1)\nRequirement already satisfied: orderedmultidict>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from furl>=2.0.0->clearml) (1.0.1)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6.0->clearml) (2023.12.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6.0->clearml) (0.18.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml) (3.7)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml) (2024.8.30)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from clearml import Task\n%env CLEARML_WEB_HOST=https://app.clear.ml/\n%env CLEARML_API_HOST=https://api.clear.ml\n%env CLEARML_FILES_HOST=https://files.clear.ml\n%env CLEARML_API_ACCESS_KEY=E2SEALD7E632F25WHX9Y4LXUO9V13K\n%env CLEARML_API_SECRET_KEY=b8tZG6KHZ-WR_Ssei-ymbOAQGQ82td8EUm37SjOXAEPRXpJBXn9MkCaMj4ZMvxyBYcU\n\ntask = Task.init(project_name=\"\", task_name=\"CT_class\")","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:31:59.225162Z","iopub.execute_input":"2024-10-03T11:31:59.225596Z","iopub.status.idle":"2024-10-03T11:32:11.208213Z","shell.execute_reply.started":"2024-10-03T11:31:59.225553Z","shell.execute_reply":"2024-10-03T11:32:11.206542Z"},"trusted":true},"outputs":[{"name":"stdout","text":"env: CLEARML_WEB_HOST=https://app.clear.ml/\nenv: CLEARML_API_HOST=https://api.clear.ml\nenv: CLEARML_FILES_HOST=https://files.clear.ml\nenv: CLEARML_API_ACCESS_KEY=E2SEALD7E632F25WHX9Y4LXUO9V13K\nenv: CLEARML_API_SECRET_KEY=b8tZG6KHZ-WR_Ssei-ymbOAQGQ82td8EUm37SjOXAEPRXpJBXn9MkCaMj4ZMvxyBYcU\n2024-10-03 11:32:03,458 - clearml.Repository Detection - WARNING - Jupyter Notebook auto-logging failed, could not access: /kaggle/working/__notebook_source__.ipynb\n","output_type":"stream"},{"name":"stderr","text":"Jupyter Notebook auto-logging failed, could not access: /kaggle/working/__notebook_source__.ipynb\n","output_type":"stream"},{"name":"stdout","text":"ClearML Task: created new task id=d1d24a82d5a643368306618f9b0b4d4b\nClearML results page: https://app.clear.ml/projects/a640a1a4c9ae4ba8be2bcb1621c14098/experiments/d1d24a82d5a643368306618f9b0b4d4b/output/log\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/clearml/utilities/process/mp.py:629: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomVerticalFlip(p=0.2),\n        transforms.RandomRotation(30),\n        transforms.ToTensor()\n\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor()\n    ])\n  \n}\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:33:23.245706Z","iopub.execute_input":"2024-10-03T11:33:23.246427Z","iopub.status.idle":"2024-10-03T11:33:23.254231Z","shell.execute_reply.started":"2024-10-03T11:33:23.246379Z","shell.execute_reply":"2024-10-03T11:33:23.252991Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class_mapping = {\n    'FRONTAL': 0,\n    'LATERAL': 1,\n    'TRASH': 2\n      }","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:33:25.714658Z","iopub.execute_input":"2024-10-03T11:33:25.715088Z","iopub.status.idle":"2024-10-03T11:33:25.720322Z","shell.execute_reply.started":"2024-10-03T11:33:25.715049Z","shell.execute_reply":"2024-10-03T11:33:25.719051Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Определяем датасет","metadata":{}},{"cell_type":"code","source":"class XRayDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n      \n       \n        \n\n        for label in os.listdir(root_dir):\n            class_dir = os.path.join(root_dir, label)\n            if os.path.isdir(class_dir):\n                for img_file in os.listdir(class_dir):\n                    img_path = os.path.join(class_dir, img_file)\n                    if img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n                        self.image_paths.append(img_path)\n                        self.labels.append(label)\n   \n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        try:        \n            image = Image.open(img_path).convert(\"RGB\")    \n        except Exception as e:        \n            print(f\"Ошибка при загрузке {img_path}: {e}\")\n            return None, None  # Возвращаем None в случае ошибки\n        if self.transform:\n            image = self.transform(image)\n\n        label = self.labels[idx]\n        \n        if label not in class_mapping:\n            print(f\"Unknown label: {label}\")\n        label_index = class_mapping[label] \n        return image, label_index\n    def get_image_id(self, img_path):\n        \"\"\"Получает имя файла (идентификатор изображения) из полного пути.\"\"\"\n        return os.path.basename(img_path)\n\n# Определяем DataModule\nclass XRayDataModule(pl.LightningDataModule):\n    def __init__(self, train_dir, val_dir, batch_size=32, num_workers=4):\n        super(XRayDataModule, self).__init__()\n        self.train_dir = train_dir\n        self.val_dir = val_dir\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.train_dataset = None\n        self.val_dataset = None\n    \n\n    def setup(self, stage=None):\n        self.train_dataset = XRayDataset(self.train_dir, transform=data_transforms['train'])\n        self.val_dataset = XRayDataset(self.val_dir, transform=data_transforms['val'])\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:33:27.762347Z","iopub.execute_input":"2024-10-03T11:33:27.762792Z","iopub.status.idle":"2024-10-03T11:33:27.778719Z","shell.execute_reply.started":"2024-10-03T11:33:27.762755Z","shell.execute_reply":"2024-10-03T11:33:27.777447Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Определяем класс классификатора","metadata":{}},{"cell_type":"code","source":"\nclass XRayClassifier(pl.LightningModule):\n    def __init__(self, num_classes=3, learning_rate=0.001):\n        super(XRayClassifier, self).__init__()\n        self.model = models.resnet18(pretrained=True)\n        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.learning_rate = learning_rate \n\n        \n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.loss_fn(outputs, labels)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = self.loss_fn(outputs, labels)\n        self.log('val_loss', loss)\n        preds = torch.argmax(outputs, dim=1)\n        acc = (preds == labels).float().mean()\n        self.log('val_accuracy', acc)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:33:31.989488Z","iopub.execute_input":"2024-10-03T11:33:31.989973Z","iopub.status.idle":"2024-10-03T11:33:32.003237Z","shell.execute_reply.started":"2024-10-03T11:33:31.989928Z","shell.execute_reply":"2024-10-03T11:33:32.001740Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#  Grid Search ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import ParameterGrid\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import TensorBoardLogger\nimport pytorch_lightning as pl\n\nparam_grid = {\n    'learning_rate': [0.0001,0.00001],\n    'batch_size': [16,32],\n    'num_epochs': [20]\n}\n\ngrid = ParameterGrid(param_grid)\n\nbest_model = None\nbest_val_loss = float('inf')\nresults = []  \n\nfor params in grid:\n    print(f\"Training with params: {params}\")\n\n    data_module = XRayDataModule(\n        train_dir='/kaggle/input/t-projection/train/train',\n        val_dir='/kaggle/input/t-projection/validation/validation',\n        batch_size=params['batch_size'],\n        num_workers=4,\n        \n    )\n    data_module.setup()\n\n    xray_classifier = XRayClassifier(num_classes=3, learning_rate=params['learning_rate'])\n    \n    # Коллбэк для ранней остановки\n    early_stopping = EarlyStopping(\n        monitor='val_loss',   # Мониторим валидационный loss\n        patience=3,           # Останавливаем обучение, если валидационный loss не улучшается в течение 3 эпох\n        mode='min',           # Мы хотим минимизировать валидационный loss\n        verbose=True\n       )\n\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',\n        filename='sample-{epoch:02d}-{val_loss:.2f}',\n        save_top_k=3,\n        mode='min',\n        dirpath='/kaggle/working/checkpoints'\n    )\n\n    tensorboard_logger = TensorBoardLogger(\"/kaggle/working/logs\", name=\"my_model\")\n\n    trainer = pl.Trainer(\n        max_epochs=params['num_epochs'],\n        callbacks=[checkpoint_callback, early_stopping ],\n        enable_progress_bar=True,\n        logger=tensorboard_logger\n    )\n\n    trainer.fit(xray_classifier, data_module)\n\n    val_loss = trainer.callback_metrics['val_loss'].item()\n    val_accuracy = trainer.callback_metrics['val_accuracy'].item()\n\n    results.append({\n        'learning_rate': params['learning_rate'],\n        'batch_size': params['batch_size'],\n        'num_epochs': params['num_epochs'],\n        'val_loss': val_loss,\n        'val_accuracy': val_accuracy,\n    })\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = xray_classifier\n\nprint(f\"Best validation loss: {best_val_loss}\")\n\nprint(\"\\nResults for all parameter combinations:\")\nfor result in results:\n    print(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:33:53.185394Z","iopub.execute_input":"2024-10-03T11:33:53.186394Z","iopub.status.idle":"2024-10-03T11:54:13.510499Z","shell.execute_reply.started":"2024-10-03T11:33:53.186349Z","shell.execute_reply":"2024-10-03T11:54:13.509360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Training with params: {'batch_size': 16, 'learning_rate': 0.0001, 'num_epochs': 20}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /kaggle/working/checkpoints exists and is not empty.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f72a59328748a7bc7b174a43fea0f8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Training with params: {'batch_size': 16, 'learning_rate': 1e-05, 'num_epochs': 20}\n","output_type":"stream"},{"name":"stderr","text":"Connecting multiple input models with the same name: `resnet18-f37072fd`. This might result in the wrong model being used when executing remotely\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e574613e078943cc90aadf0134d6c413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Training with params: {'batch_size': 32, 'learning_rate': 0.0001, 'num_epochs': 20}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334ee9b2cffd468f883d594560e6d6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Training with params: {'batch_size': 32, 'learning_rate': 1e-05, 'num_epochs': 20}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63e729375be142f38a306e314fa5afe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Best validation loss: 0.18988138437271118\n\nResults for all parameter combinations:\n{'learning_rate': 0.0001, 'batch_size': 16, 'num_epochs': 20, 'val_loss': 0.27968981862068176, 'val_accuracy': 0.9039039015769958}\n{'learning_rate': 1e-05, 'batch_size': 16, 'num_epochs': 20, 'val_loss': 0.21972712874412537, 'val_accuracy': 0.9029029011726379}\n{'learning_rate': 0.0001, 'batch_size': 32, 'num_epochs': 20, 'val_loss': 0.3064603805541992, 'val_accuracy': 0.8958958983421326}\n{'learning_rate': 1e-05, 'batch_size': 32, 'num_epochs': 20, 'val_loss': 0.18988138437271118, 'val_accuracy': 0.9119119048118591}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Лучшее значение loss val: 0.062\nКомбинация гиперпараметров:\nlearning_rate: 0.0001\nbatch_size: 32\nnum_epochs: 20\nТочность: 0.9819","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"best_model","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:54:59.685222Z","iopub.execute_input":"2024-10-03T11:54:59.686125Z","iopub.status.idle":"2024-10-03T11:54:59.696063Z","shell.execute_reply.started":"2024-10-03T11:54:59.686079Z","shell.execute_reply":"2024-10-03T11:54:59.694927Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"XRayClassifier(\n  (model): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=512, out_features=3, bias=True)\n  )\n  (loss_fn): CrossEntropyLoss()\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Сохраним веса","metadata":{}},{"cell_type":"code","source":"best_model_weights_path = '/kaggle/working/best_model_weights.pth'\ntorch.save(best_model.state_dict(), best_model_weights_path)\nprint(f\"Weights saved to {best_model_weights_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:55:04.457279Z","iopub.execute_input":"2024-10-03T11:55:04.458255Z","iopub.status.idle":"2024-10-03T11:55:07.771957Z","shell.execute_reply.started":"2024-10-03T11:55:04.458209Z","shell.execute_reply":"2024-10-03T11:55:07.770942Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Weights saved to /kaggle/working/best_model_weights.pth\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"best_model_path = '/kaggle/working/best_model.pth'\ntorch.save(best_model, best_model_path)\nprint(f\"Model saved to {best_model_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:55:09.040133Z","iopub.execute_input":"2024-10-03T11:55:09.041099Z","iopub.status.idle":"2024-10-03T11:55:12.334113Z","shell.execute_reply.started":"2024-10-03T11:55:09.041052Z","shell.execute_reply":"2024-10-03T11:55:12.332953Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/best_model.pth\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def visualize_data(data_module, num_images=16):\n    train_loader = data_module.train_dataloader()\n    images, labels = next(iter(train_loader))\n    class_mapping = {0: 'FRONTAL', 1: 'LATERAL', 2: 'TRASH'}\n    labels = [class_mapping[label.item()] for label in labels]\n    plt.figure(figsize=(15, 10))\n    for i in range(num_images):\n        plt.subplot(4, 4, i + 1)\n        plt.imshow(images[i].permute(1, 2, 0))  # Преобразуем тензор (C, H, W) в (H, W, C)\n        plt.title(labels[i])\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nvisualize_data(data_module)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\ndef check_class_distribution(data_module):\n    train_loader = data_module.train_dataloader()\n    labels = []\n    for _, batch_labels in train_loader:\n        labels.extend(batch_labels.cpu().numpy())\n\n    class_count = Counter(labels)\n    print(\"Распределение классов в тренировочном наборе:\")\n    for class_label, count in class_count.items():\n        print(f'Класс {class_label}: {count} изображений')\ncheck_class_distribution(data_module)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Validation результаты","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Загрузка модели\nmodel = XRayClassifier(num_classes=3)  # Создаем экземпляр модели\nbest_model_path = '/kaggle/input/test15/pytorch/default/1/best_model_weights.pth'\n\n# Загрузка сохраненных весов\nmodel.load_state_dict(torch.load(best_model_path))\n\n# Перемещаем модель на устройство (GPU или CPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n# Перевод модели в режим вывода (inference)\nmodel.eval()\n\n# Валидационный лоадер\nvalidation_loader = data_module.val_dataloader()\n\ndef inference_and_save_csv(model, dataloader, output_file):\n    model.eval()  # Перевод модели в режим вывода\n    results = []\n    \n    dataset = dataloader.dataset\n\n    with torch.no_grad():\n        for batch_idx, (images, labels) in enumerate(tqdm(dataloader)):  # Обертка с tqdm для отслеживания прогресса\n            images = images.to(device)\n\n            outputs = model(images)\n            activations = torch.softmax(outputs, dim=1).cpu().numpy()\n            \n            for i in range(len(images)):\n                image_id = dataset.get_image_id(dataset.image_paths[batch_idx * dataloader.batch_size + i])\n                \n                results.append({\n                    \"image_id\": image_id,\n                    \"frontal\": activations[i][0],\n                    \"lateral\": activations[i][1],\n                    \"trash\": activations[i][2]\n                })\n    \n    print(f\"Всего результатов для сохранения: {len(results)}\")\n    \n    df = pd.DataFrame(results)\n    df.to_csv(output_file, index=False, header=True)\n    print(f\"Результаты сохранены в {output_file}\")\n\noutput_csv_path = \"/kaggle/working/classification_results.csv\"\ninference_and_save_csv(model, validation_loader, output_csv_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T11:57:26.512921Z","iopub.execute_input":"2024-10-03T11:57:26.513895Z","iopub.status.idle":"2024-10-03T11:57:59.292381Z","shell.execute_reply.started":"2024-10-03T11:57:26.513850Z","shell.execute_reply":"2024-10-03T11:57:59.291098Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n\n/opt/conda/lib/python3.10/site-packages/clearml/binding/frameworks/pytorch_bind.py:277: FutureWarning:\n\nYou are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n\nConnecting multiple input models with the same name: `best_model_weights`. This might result in the wrong model being used when executing remotely\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771a323d32c34c4baf1790080e5fe117"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning:\n\nos.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n\n","output_type":"stream"},{"name":"stdout","text":"Всего результатов для сохранения: 999\nРезультаты сохранены в /kaggle/working/classification_results.csv\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Выявим ошибки","metadata":{}},{"cell_type":"code","source":"\nfrontal_dir = '/kaggle/input/t-projection/validation/validation/FRONTAL'\nlateral_dir = '/kaggle/input/t-projection/validation/validation/LATERAL'\ntrash_dir = '/kaggle/input/t-projection/validation/validation/TRASH'\n\n\ntrue_labels = {}\n\n\nfor label, directory in zip(['frontal', 'lateral', 'trash'], [frontal_dir, lateral_dir, trash_dir]):\n    for filename in os.listdir(directory):\n        if filename.endswith('.png'):\n            true_labels[filename] = label\n\n\npredictions_df = pd.read_csv(output_csv_path)\n\n\nincorrect_count = 0\n\nfor _, row in predictions_df.iterrows():\n    image_id = row['image_id']\n    predicted_label = 'frontal' if row['frontal'] > max(row['lateral'], row['trash']) else ('lateral' if row['lateral'] > row['trash'] else 'trash')\n    \n    true_label = true_labels.get(image_id, None)\n    \n    if true_label and predicted_label != true_label:\n        incorrect_count += 1\n\nprint(f\"Количество неверных предсказаний: {incorrect_count}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T12:03:09.153926Z","iopub.execute_input":"2024-10-03T12:03:09.154401Z","iopub.status.idle":"2024-10-03T12:03:09.274368Z","shell.execute_reply.started":"2024-10-03T12:03:09.154356Z","shell.execute_reply":"2024-10-03T12:03:09.273113Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Количество неверных предсказаний: 15\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Пути к директориям с изображениями\nfrontal_dir = '/kaggle/input/t-projection/validation/validation/FRONTAL'\nlateral_dir = '/kaggle/input/t-projection/validation/validation/LATERAL'\ntrash_dir = '/kaggle/input/t-projection/validation/validation/TRASH'\n\ntrue_labels = {}\n\nfor label, directory in zip(['frontal', 'lateral', 'trash'], [frontal_dir, lateral_dir, trash_dir]):\n    for filename in os.listdir(directory):\n        if filename.endswith('.png'):\n            true_labels[filename] = label\n\npredictions_df = pd.read_csv(output_csv_path)\n\nincorrect_predictions = []\n\nfor _, row in predictions_df.iterrows():\n    image_id = row['image_id']\n    predicted_label = 'frontal' if row['frontal'] > max(row['lateral'], row['trash']) else ('lateral' if row['lateral'] > row['trash'] else 'trash')\n    \n    true_label = true_labels.get(image_id, None)\n    \n    if true_label and predicted_label != true_label:\n        incorrect_predictions.append((image_id, true_label, predicted_label))\n\nnum_images = min(len(incorrect_predictions), 10)  # Ограничение до 10 изображений\nplt.figure(figsize=(15, 7 * (num_images // 2 + 1)))\n\nfor i, (image_id, true_label, predicted_label) in enumerate(incorrect_predictions[:num_images]):\n    image_path = os.path.join(frontal_dir if true_label == 'frontal' else lateral_dir if true_label == 'lateral' else trash_dir, image_id)\n    image = Image.open(image_path)\n\n    plt.subplot(num_images // 3 + 1, 3, i + 1)\n    plt.imshow(image)\n    plt.title(f\"True: {true_label}, Pred: {predicted_label}\\nFilename: {image_id}\")\n    plt.axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{},"outputs":[],"execution_count":null}]}